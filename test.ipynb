{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer, models, losses, InputExample\n",
    "from torch import nn, Tensor\n",
    "from typing import List, Union, Iterable, Dict\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import dataclasses\n",
    "from pathlib import Path\n",
    "from typing import List\n",
    "import pandas as pd\n",
    "from itertools import count\n",
    "import typer\n",
    "from enum import Enum\n",
    "from typing import Optional\n",
    "from collections import defaultdict\n",
    "import torch\n",
    "import numpy as np\n",
    "from sentence_transformers.util import cos_sim\n",
    "from sentence_transformers import SentenceTransformer, InputExample, losses\n",
    "from dos.evaluator import CorrelationEvaluator\n",
    "from dos.dataset import SemEvalDataset, ArticlePair\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from dos.multitask_evaluator import MultitaskCorrelationEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import Tensor\n",
    "from torch import nn\n",
    "from typing import Dict\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class ReshapeAndNormalize(nn.Module):\n",
    "    \"\"\"\n",
    "    This layer normalizes embeddings to unit length\n",
    "    \"\"\"\n",
    "    def __init__(self, num_labels: int):\n",
    "        super(ReshapeAndNormalize, self).__init__()\n",
    "        self.num_labels = num_labels\n",
    "\n",
    "    def forward(self, features: Dict[str, Tensor]):\n",
    "        batch_size, _ = features['sentence_embedding'].shape\n",
    "        features.update({'sentence_embedding': features['sentence_embedding'].reshape(batch_size, self.num_labels, -1)})\n",
    "        features.update({'sentence_embedding': F.normalize(features['sentence_embedding'], p=2, dim=2)})\n",
    "        return features\n",
    "\n",
    "    def save(self, output_path):\n",
    "        pass\n",
    "\n",
    "    @staticmethod\n",
    "    def load(input_path):\n",
    "        return ReshapeAndNormalize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SentenceTransformer(\"sentence-transformers/LaBSE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = [\"Tim ist toll\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 768)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_embeddings = model.encode(sentences=sentences)\n",
    "sentence_embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SentenceTransformer(\n",
       "  (0): Transformer({'max_seq_length': 256, 'do_lower_case': False}) with Transformer model: BertModel \n",
       "  (1): Pooling({'word_embedding_dimension': 768, 'pooling_mode_cls_token': True, 'pooling_mode_mean_tokens': False, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False})\n",
       "  (2): Dense({'in_features': 768, 'out_features': 768, 'bias': True, 'activation_function': 'torch.nn.modules.activation.Tanh'})\n",
       "  (3): Dense({'in_features': 768, 'out_features': 3584, 'bias': True, 'activation_function': 'torch.nn.modules.activation.Tanh'})\n",
       "  (4): ReshapeAndNormalize()\n",
       ")"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.add_module(\"3\", models.Dense(in_features=768, out_features=512*7, activation_function=nn.Tanh()))\n",
    "model.add_module(\"4\", ReshapeAndNormalize(num_labels=7))\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 7, 512)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_embeddings = model.encode(sentences=sentences)\n",
    "sentence_embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.05088175, -0.05160476,  0.05517431,  0.011707  , -0.00567881,\n",
       "        0.03671409,  0.0588317 , -0.07588776,  0.07630678,  0.01865661,\n",
       "        0.01614934,  0.06369347,  0.05437566,  0.04573026, -0.01723144,\n",
       "        0.00204056,  0.0681881 , -0.05109839,  0.0934725 , -0.06399766,\n",
       "        0.02728841,  0.01521847,  0.01204677, -0.01487857, -0.01405284,\n",
       "       -0.05299329, -0.06663571, -0.06151884, -0.01764604,  0.02136873,\n",
       "        0.00298715, -0.04767498,  0.0025035 , -0.00124061, -0.01686211,\n",
       "       -0.01935672,  0.02121147,  0.03551834, -0.03067654,  0.00992155,\n",
       "       -0.03848063,  0.0081587 ,  0.02219091, -0.01393551,  0.04945447,\n",
       "        0.10523274,  0.04523982, -0.05995491,  0.06702071,  0.0300389 ,\n",
       "        0.06169705, -0.01010427, -0.03548419,  0.00346366, -0.02165526,\n",
       "        0.01710977, -0.02390138, -0.02049441,  0.0579266 ,  0.10538305,\n",
       "        0.00716479,  0.0168098 , -0.07584294,  0.08076446, -0.0028929 ,\n",
       "        0.06764733, -0.03247941, -0.04045565, -0.00938718, -0.04218852,\n",
       "        0.00356972, -0.0608115 , -0.07928877,  0.00621879,  0.00574176,\n",
       "        0.03670723,  0.02245674,  0.04058835, -0.02200993,  0.06637544,\n",
       "        0.02522419,  0.03311136, -0.02175446, -0.01396978,  0.00224277,\n",
       "        0.00416819, -0.08951543, -0.02407223,  0.01658478, -0.02691365,\n",
       "       -0.00863943, -0.027134  , -0.02461399,  0.06256733,  0.02325632,\n",
       "        0.05398072, -0.03962102,  0.05172741,  0.05847899,  0.02058655,\n",
       "        0.05092224,  0.01610975,  0.0579935 , -0.00723896,  0.06192343,\n",
       "       -0.03586257,  0.0253602 , -0.0138431 , -0.03420243, -0.00813677,\n",
       "        0.05945607, -0.03851908, -0.02255793, -0.00643601, -0.03217287,\n",
       "        0.0035112 ,  0.04939515, -0.0122012 ,  0.01600239, -0.00542742,\n",
       "       -0.02018198, -0.06758783, -0.05503224, -0.01653248, -0.03435141,\n",
       "        0.01758885, -0.00754966,  0.02848217, -0.06037381,  0.04768459,\n",
       "        0.03760495, -0.0406884 ,  0.01954554,  0.0147022 , -0.00893362,\n",
       "        0.03130987,  0.02436192, -0.04939087,  0.05617662,  0.01869481,\n",
       "       -0.01519347, -0.01183071,  0.05740569,  0.05829068,  0.02475037,\n",
       "       -0.02239986, -0.06234846, -0.01970136, -0.00639376, -0.04676577,\n",
       "        0.0116931 , -0.04621258, -0.05852165,  0.06014077, -0.03027691,\n",
       "        0.02432268,  0.07426018,  0.02129506, -0.02778652,  0.08093525,\n",
       "       -0.11077905, -0.00182931,  0.01833618, -0.06908926, -0.07966959,\n",
       "        0.04073817,  0.02814426,  0.03139167,  0.03386153, -0.03368813,\n",
       "        0.02074164, -0.0223813 ,  0.00640124, -0.10028696,  0.01906427,\n",
       "        0.01467815,  0.01013229, -0.0770654 ,  0.0066499 , -0.02472507,\n",
       "        0.08340311, -0.0364895 , -0.09657016, -0.02175382, -0.01092617,\n",
       "       -0.09897183, -0.01353397, -0.0412198 ,  0.01408331, -0.01645487,\n",
       "        0.02183163, -0.01980031,  0.03694099,  0.08403932, -0.03226726,\n",
       "        0.01187831,  0.03147337,  0.01548247,  0.04035449, -0.06802197,\n",
       "        0.01930548,  0.04316665,  0.06162065, -0.03138335,  0.05298142,\n",
       "       -0.09143076, -0.00448603, -0.01977772,  0.01490372,  0.01725597,\n",
       "        0.0256686 , -0.0677251 , -0.03478077,  0.024289  , -0.0374272 ,\n",
       "        0.00676745, -0.03447982, -0.05124404, -0.02255385,  0.04921251,\n",
       "        0.01103495, -0.05572723, -0.00362904, -0.04578121,  0.00671543,\n",
       "       -0.02917226, -0.02592316, -0.04352672, -0.00021402,  0.04713832,\n",
       "        0.04301456, -0.00766692, -0.04551202, -0.1067422 ,  0.01303401,\n",
       "       -0.02617378,  0.04025247, -0.0345167 ,  0.02112976,  0.00784051,\n",
       "       -0.01160908,  0.0157291 ,  0.02517132,  0.04390116, -0.03976743,\n",
       "        0.03774914,  0.01733555, -0.03360153, -0.00882441,  0.00637041,\n",
       "        0.1104603 ,  0.02807456, -0.00813184,  0.08768766,  0.03598399,\n",
       "        0.00301713,  0.01551574, -0.01918755,  0.05742158,  0.00411151,\n",
       "        0.06647035,  0.00272239, -0.05452723, -0.05592368,  0.04044703,\n",
       "       -0.01288056,  0.00149176,  0.07281733, -0.010683  ,  0.00779944,\n",
       "       -0.05794787, -0.04018993,  0.033702  , -0.08870494,  0.01785588,\n",
       "        0.04324995,  0.01622754, -0.07165791,  0.01643817, -0.03648941,\n",
       "        0.02055152,  0.01290344,  0.00724859,  0.04987375,  0.02615465,\n",
       "       -0.03437964,  0.02167064, -0.00404072,  0.04218178, -0.09106278,\n",
       "       -0.01163938,  0.00678966,  0.03334488,  0.03991311,  0.04111721,\n",
       "       -0.0149673 , -0.02873298, -0.0127678 ,  0.02543163, -0.00101759,\n",
       "       -0.06747278, -0.01261104, -0.01386201,  0.06798336,  0.05108967,\n",
       "       -0.01717619,  0.00255837, -0.006277  ,  0.02443969, -0.02178277,\n",
       "        0.04451988,  0.05800854,  0.0175838 , -0.0436633 , -0.0813098 ,\n",
       "        0.03704112, -0.02976863,  0.05883351, -0.0210581 ,  0.02896272,\n",
       "        0.0180797 , -0.0598655 ,  0.05190865,  0.01234031,  0.02805376,\n",
       "       -0.01231451, -0.01583642, -0.01543356, -0.08765786,  0.04595692,\n",
       "       -0.0111491 , -0.02923621,  0.09875254,  0.05512978,  0.02337941,\n",
       "        0.05278613, -0.01043962, -0.00734745, -0.00313767,  0.00953715,\n",
       "        0.03896213, -0.02331459,  0.07470234,  0.00872441, -0.02108014,\n",
       "       -0.07470022, -0.00590439,  0.07242203, -0.04802253,  0.02392062,\n",
       "       -0.03450479,  0.0196488 , -0.01485834, -0.02804313,  0.04137389,\n",
       "        0.08269857, -0.02220307,  0.01880434, -0.06763864, -0.06033872,\n",
       "       -0.02750383, -0.01929239, -0.10555442, -0.03538752,  0.04709788,\n",
       "       -0.01395163, -0.01162737, -0.02786528,  0.00036608,  0.01841252,\n",
       "        0.05550005, -0.01389556, -0.01137325,  0.03501722,  0.07861126,\n",
       "       -0.03759525,  0.05695382, -0.04379601, -0.02471845,  0.06728823,\n",
       "       -0.07760695,  0.01274594,  0.0067811 , -0.04646156,  0.05211607,\n",
       "        0.04735758,  0.0700746 ,  0.07816327,  0.04551199, -0.05145286,\n",
       "        0.11169727, -0.02528906, -0.04153028,  0.04596489,  0.05749963,\n",
       "        0.03282264, -0.02242618,  0.02486454, -0.07071222, -0.04840772,\n",
       "       -0.03169426,  0.0181075 , -0.0449412 , -0.00952448,  0.03156487,\n",
       "        0.12926926,  0.11557153,  0.03045512,  0.00107198, -0.08325995,\n",
       "       -0.01559949,  0.03686422,  0.01259412,  0.02254283, -0.02515225,\n",
       "       -0.00283427,  0.02560332,  0.00643427, -0.05890305, -0.05714512,\n",
       "       -0.01903403, -0.09577788,  0.03284931,  0.02998802, -0.06055135,\n",
       "       -0.04217991,  0.03930276,  0.00783969, -0.01791215, -0.03427624,\n",
       "        0.06518619, -0.0501718 , -0.02685176,  0.06950236, -0.04624501,\n",
       "        0.04722485, -0.01228106, -0.00028398,  0.03924445,  0.01646821,\n",
       "       -0.11908296, -0.06052009, -0.04158532, -0.03802632,  0.02035072,\n",
       "        0.06468434,  0.03765208, -0.01374723,  0.04086678, -0.02632863,\n",
       "       -0.02219146,  0.07980902, -0.06419917,  0.01317517,  0.07556666,\n",
       "        0.02412857,  0.03565587,  0.02184122,  0.01660328, -0.06167379,\n",
       "       -0.05630512, -0.00826027,  0.00811837,  0.06946681, -0.00276991,\n",
       "       -0.03757574,  0.06057224, -0.05059607, -0.00487951, -0.00958339,\n",
       "        0.06040843, -0.0806482 ,  0.01513923,  0.01607756, -0.07576252,\n",
       "        0.04683987,  0.01146005, -0.0005357 ,  0.02014578, -0.01762508,\n",
       "       -0.02165026, -0.03870737,  0.02595146,  0.02758902, -0.08364974,\n",
       "        0.01382822,  0.06556363, -0.0169753 ,  0.0220368 , -0.03014004,\n",
       "        0.03145657,  0.05308633,  0.01274842, -0.05759969, -0.0550944 ,\n",
       "        0.03648034, -0.0131706 , -0.05293161, -0.03038832,  0.01132261,\n",
       "        0.02048115,  0.10264981, -0.08677594, -0.02757667,  0.00205818,\n",
       "        0.07009961, -0.00172877, -0.07357499, -0.04016261, -0.06178348,\n",
       "        0.06274366, -0.01199222], dtype=float32)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_embeddings[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InputExampleWithMultipleLabels:\n",
    "    \"\"\"\n",
    "    Structure for one input example with texts, the label and a unique id\n",
    "    \"\"\"\n",
    "    def __init__(self, guid: str = '', texts: List[str] = None,  label: List[Union[int, float]] = 0):\n",
    "        \"\"\"\n",
    "        Creates one InputExample with the given texts, guid and label\n",
    "\n",
    "\n",
    "        :param guid\n",
    "            id for the example\n",
    "        :param texts\n",
    "            the texts for the example.\n",
    "        :param label\n",
    "            the label for the example\n",
    "        \"\"\"\n",
    "        self.guid = guid\n",
    "        self.texts = texts\n",
    "        self.label = label\n",
    "\n",
    "    def __str__(self):\n",
    "        return \"<InputExample> labels: {}, texts: {}\".format(\"; \".join(self.label), \"; \".join(self.texts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CosineSimilarityLossForMultipleLabels(nn.Module):\n",
    "\n",
    "    def __init__(self, model: SentenceTransformer, loss_fct = nn.MSELoss(), cos_score_transformation=nn.Identity(), num_labels=7):\n",
    "        super(CosineSimilarityLossForMultipleLabels, self).__init__()\n",
    "        self.model = model\n",
    "        self.loss_fct = loss_fct\n",
    "        self.cos_score_transformation = cos_score_transformation\n",
    "        self.num_labels = num_labels\n",
    "\n",
    "\n",
    "    def forward(self, sentence_features: Iterable[Dict[str, Tensor]], labels: Tensor):\n",
    "        embeddings = [self.model(sentence_feature)['sentence_embedding'] for sentence_feature in sentence_features]\n",
    "        output = torch.stack([self.cos_score_transformation(torch.cosine_similarity(embeddings[0][:,dim,:], embeddings[1][:,dim,:])) for dim in range(self.num_labels)]).T\n",
    "        return self.loss_fct(output, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_score_01(one2four: float):\n",
    "    return ((1 - 2 * (one2four - 1) / 3) + 1) / 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_training_data(data: List[ArticlePair]) -> List[InputExample]:\n",
    "    inputs: List[InputExample] = [\n",
    "        InputExampleWithMultipleLabels(\n",
    "            texts=[pair.article_1.text, pair.article_2.text],\n",
    "            label=[\n",
    "                normalize_score_01(pair.geography), \n",
    "                normalize_score_01(pair.entities), \n",
    "                normalize_score_01(pair.time), \n",
    "                normalize_score_01(pair.narrative), \n",
    "                normalize_score_01(pair.overall), \n",
    "                normalize_score_01(pair.style), \n",
    "                normalize_score_01(pair.tone)\n",
    "            ]\n",
    "        )\n",
    "        for pair in data\n",
    "    ]\n",
    "    return inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error loading article No articles with id 1484008954\n",
      "Error loading article No articles with id 1483999027\n",
      "Error loading article No articles with id 1484182933\n",
      "Error loading article No articles with id 1483999027\n",
      "Error loading article No articles with id 1484035175\n",
      "Error loading article No articles with id 1484007787\n",
      "Error loading article No articles with id 1484007543\n",
      "Error loading article No articles with id 1484370272\n",
      "Error loading article No articles with id 1484368406\n",
      "Error loading article No articles with id 1484370272\n",
      "Error loading article No articles with id 1483806257\n",
      "Error loading article No articles with id 1484038154\n",
      "Error loading article No articles with id 1484191176\n",
      "Error loading article No articles with id 1484040035\n",
      "Error loading article No articles with id 1484452087\n",
      "Error loading article No articles with id 1483999027\n",
      "Error loading article No articles with id 1484200356\n",
      "Error loading article No articles with id 1484188519\n",
      "Error loading article No articles with id 1484155494\n",
      "Error loading article No articles with id 1483937746\n",
      "Error loading article No articles with id 1484043880\n",
      "Error loading article No articles with id 1484368406\n",
      "Error loading article No articles with id 1484010524\n",
      "Error loading article No articles with id 1484759952\n",
      "Error loading article No articles with id 1484206125\n",
      "Error loading article No articles with id 1484206125\n",
      "Error loading article No articles with id 1484209302\n",
      "Error loading article No articles with id 1484180955\n",
      "Error loading article No articles with id 1508535263\n",
      "Error loading article No articles with id 1552509332\n",
      "Error loading article No articles with id 1497072555\n",
      "Error loading article No articles with id 1510347990\n",
      "Error loading article No articles with id 1556264775\n",
      "Error loading article No articles with id 1520859235\n",
      "Error loading article No articles with id 1537006701\n",
      "Error loading article No articles with id 1505204186\n",
      "Error loading article No articles with id 1484753436\n",
      "Error loading article No articles with id 1495587978\n",
      "Error loading article No articles with id 1527457673\n",
      "Error loading article No articles with id 1622743303\n",
      "Error loading article No articles with id 1614550354\n",
      "Error loading article No articles with id 1605389491\n",
      "Error loading article No articles with id 1533838523\n",
      "Error loading article No articles with id 1545766920\n",
      "Error loading article No articles with id 1622443827\n",
      "Error loading article No articles with id 1572033318\n",
      "Error loading article No articles with id 1520703164\n",
      "Error loading article No articles with id 1586205249\n",
      "Error loading article No articles with id 1603728153\n",
      "Error loading article No articles with id 1551792591\n",
      "Error loading article No articles with id 1586205249\n"
     ]
    }
   ],
   "source": [
    "dataset = SemEvalDataset(Path(\"data/train.csv\"), Path(\"data/train_data\"))\n",
    "train, dev = dataset.random_split(0.8)\n",
    "training_inputs = make_training_data(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = DataLoader(training_inputs, shuffle=True, batch_size=8)\n",
    "loss = CosineSimilarityLossForMultipleLabels(model, num_labels=7)\n",
    "evaluator = CorrelationEvaluator(dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2c7e4d9891594fcba68a20c4b7bea685",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "20f635f4688f48f09a4f09dbb730fe60",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Iteration:   0%|          | 0/492 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 7])\n",
      "torch.Size([8, 7])\n",
      "torch.Size([8, 7])\n",
      "torch.Size([8, 7])\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 1.44 GiB (GPU 0; 23.69 GiB total capacity; 13.53 GiB already allocated; 234.69 MiB free; 15.16 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[33], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m model\u001b[39m.\u001b[39;49mfit(\n\u001b[1;32m      2\u001b[0m     train_objectives\u001b[39m=\u001b[39;49m[(dataloader, loss)],\n\u001b[1;32m      3\u001b[0m     epochs\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m,\n\u001b[1;32m      4\u001b[0m     warmup_steps\u001b[39m=\u001b[39;49m\u001b[39m100\u001b[39;49m,\n\u001b[1;32m      5\u001b[0m     evaluator\u001b[39m=\u001b[39;49mevaluator,\n\u001b[1;32m      6\u001b[0m     use_amp\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m      7\u001b[0m     output_path\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mmodels\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m      8\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda3/envs/ml/lib/python3.10/site-packages/sentence_transformers/SentenceTransformer.py:713\u001b[0m, in \u001b[0;36mSentenceTransformer.fit\u001b[0;34m(self, train_objectives, evaluator, epochs, steps_per_epoch, scheduler, warmup_steps, optimizer_class, optimizer_params, weight_decay, evaluation_steps, output_path, save_best_model, max_grad_norm, use_amp, callback, show_progress_bar, checkpoint_path, checkpoint_save_steps, checkpoint_save_total_limit)\u001b[0m\n\u001b[1;32m    710\u001b[0m     loss_value \u001b[39m=\u001b[39m loss_model(features, labels)\n\u001b[1;32m    712\u001b[0m scale_before_step \u001b[39m=\u001b[39m scaler\u001b[39m.\u001b[39mget_scale()\n\u001b[0;32m--> 713\u001b[0m scaler\u001b[39m.\u001b[39;49mscale(loss_value)\u001b[39m.\u001b[39;49mbackward()\n\u001b[1;32m    714\u001b[0m scaler\u001b[39m.\u001b[39munscale_(optimizer)\n\u001b[1;32m    715\u001b[0m torch\u001b[39m.\u001b[39mnn\u001b[39m.\u001b[39mutils\u001b[39m.\u001b[39mclip_grad_norm_(loss_model\u001b[39m.\u001b[39mparameters(), max_grad_norm)\n",
      "File \u001b[0;32m~/miniconda3/envs/ml/lib/python3.10/site-packages/torch/_tensor.py:487\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    477\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[1;32m    478\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    479\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[1;32m    480\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    485\u001b[0m         inputs\u001b[39m=\u001b[39minputs,\n\u001b[1;32m    486\u001b[0m     )\n\u001b[0;32m--> 487\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\n\u001b[1;32m    488\u001b[0m     \u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs\n\u001b[1;32m    489\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda3/envs/ml/lib/python3.10/site-packages/torch/autograd/__init__.py:197\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    192\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[1;32m    194\u001b[0m \u001b[39m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    195\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    196\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 197\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    198\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[1;32m    199\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 1.44 GiB (GPU 0; 23.69 GiB total capacity; 13.53 GiB already allocated; 234.69 MiB free; 15.16 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "model.fit(\n",
    "    train_objectives=[(dataloader, loss)],\n",
    "    epochs=1,\n",
    "    warmup_steps=100,\n",
    "    evaluator=evaluator,\n",
    "    use_amp=True,\n",
    "    output_path=\"models\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Normalize()"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "models.Normalize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models.Dense()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "F.normalize(features['sentence_embedding'], p=2, dim=1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "6ade12b733bf20346d3a1a61bfa94628264257fe497efa0a51269291fae6908e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
